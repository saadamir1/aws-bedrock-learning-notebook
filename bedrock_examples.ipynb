{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f73937a-f6f2-4810-863d-0cce22b09ad2",
   "metadata": {},
   "source": [
    "### **Purpose:** Demonstrates basic text generation using the **Amazon Bedrock Runtime API**.  \n",
    "This code sends a simple prompt to a foundation model (**Titan Text Express**) and retrieves the generated response.  \n",
    "It shows how to directly invoke a model with a single user prompt using `invoke_model` (or `converse`) without maintaining conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec88705c-932b-4494-b3c8-680903e5e3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\n\\nDevOps is a methodology that combines software development and IT operations to shorten the systems development life cycle while delivering features, enhancements, and fixes quicker.\\n\"\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create Bedrock Runtime client\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "response = bedrock.converse(\n",
    "    modelId=\"amazon.titan-text-express-v1\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": [{\"text\": \"Summarize what DevOps is in one line.\"}]}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(json.dumps(response[\"output\"][\"message\"][\"content\"][0][\"text\"], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22218c42-68b8-4516-bf06-705c68e298fb",
   "metadata": {},
   "source": [
    "### **Purpose:** Demonstrates **Retrieval-Augmented Generation (RAG)** using the **Amazon Bedrock Agent Runtime API**.  \n",
    "The code retrieves relevant context from a connected **Knowledge Base** (stored in an **S3 bucket**) and generates a contextual response using the **Claude model**.  \n",
    "It illustrates how Bedrock combines **retrieval (from stored documents)** and **generation (via LLMs)** to answer domain-specific queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14a724-c03a-40f9-ba33-3a46cd13dbf0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Output:\n",
      "\n",
      "\"To restart the staging environment, you should run the script `restart-staging.sh` on the staging server.\"\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create Bedrock Agent Runtime client\n",
    "agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "response = agent_runtime.retrieve_and_generate(\n",
    "    input={\"text\": \"How do I restart the staging environment?\"},\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            \"knowledgeBaseId\": \"YOUR_KNOWLEDGE_BASE_ID_HERE\",  # Update with your KB ID\n",
    "            \"modelArn\": \"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nModel Output:\\n\")\n",
    "print(json.dumps(response[\"output\"][\"text\"], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc6bd6-1dbd-4ab6-8cd3-491a23c5f8e0",
   "metadata": {},
   "source": [
    "### **Purpose:** Demonstrates how to build a **multi-turn conversational chatbot** using the **Amazon Bedrock Converse API**.  \n",
    "This example defines a **system prompt** (role/persona) and maintains **conversation history** across multiple user messages.  \n",
    "It shows how to manage dialogue context and model responses using parameters like `temperature`, `topP`, and `maxTokens` for creative yet coherent outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14d71157-aa65-4914-94cd-4ba3a3aad10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- First Response ---\n",
      "Here are three rock songs:\n",
      "\n",
      "1. \"Smells Like Teen Spirit\" by Nirvana\n",
      "2. \"Sweet Child O' Mine\" by Guns N' Roses\n",
      "3. \"Bohemian Rhapsody\" by Queen\n",
      "\n",
      "--- Second Response ---\n",
      "Okay, here are three rock songs by artists from the United Kingdom:\n",
      "\n",
      "1. \"Satisfaction\" by The Rolling Stones\n",
      "2. \"Paranoid\" by Black Sabbath\n",
      "3. \"Layla\" by Derek and the Dominos\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Create a Bedrock Runtime client\n",
    "# -----------------------------------------------------------\n",
    "# Used to send requests to Amazon Bedrock models.\n",
    "# Make sure AWS credentials are configured and region is correct.\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# The Claude model we‚Äôll use for text generation\n",
    "MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Define the system prompt (model‚Äôs role/personality)\n",
    "# -----------------------------------------------------------\n",
    "# This tells the model *who it is* and *how to behave*.\n",
    "SYSTEM_PROMPT = [\n",
    "    {\n",
    "        \"text\": (\n",
    "            \"You are an app that creates playlists for a radio station \"\n",
    "            \"that plays rock and pop music. Only return song names and artists.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Set inference parameters (control creativity)\n",
    "# -----------------------------------------------------------\n",
    "# temperature ‚Üí randomness / creativity level (0 = focused, 1 = creative)\n",
    "# topP        ‚Üí limits how random tokens are chosen (nucleus sampling)\n",
    "# maxTokens   ‚Üí caps how long the response can be\n",
    "INFERENCE_CONFIG = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"topP\": 0.9,\n",
    "    \"maxTokens\": 512\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Helper function: send request to model\n",
    "# -----------------------------------------------------------\n",
    "def ask_model(messages):\n",
    "    \"\"\"\n",
    "    Sends conversation history (messages) to the model and\n",
    "    returns only the generated text response.\n",
    "    \"\"\"\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        system=SYSTEM_PROMPT,\n",
    "        messages=messages,\n",
    "        inferenceConfig=INFERENCE_CONFIG,\n",
    "    )\n",
    "    return response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Main conversation flow\n",
    "# -----------------------------------------------------------\n",
    "def main():\n",
    "    # Step 1: user starts conversation\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"text\": \"Create a list of three rock songs.\"}]}\n",
    "    ]\n",
    "\n",
    "    # Step 2: get first model reply\n",
    "    first_reply = ask_model(messages)\n",
    "    print(\"\\n--- First Response ---\")\n",
    "    print(first_reply)\n",
    "\n",
    "    # Step 3: add model‚Äôs reply back into conversation history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": [{\"text\": first_reply}]})\n",
    "\n",
    "    # Step 4: ask a follow-up question (with context)\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"Make sure the songs are by artists from the United Kingdom.\"}]\n",
    "    })\n",
    "\n",
    "    # Step 5: get contextual (second) response\n",
    "    second_reply = ask_model(messages)\n",
    "    print(\"\\n--- Second Response ---\")\n",
    "    print(second_reply)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6Ô∏è‚É£ Entry point (run main)\n",
    "# -----------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3249fc0d-f664-43c7-95e4-7acae762ea3e",
   "metadata": {},
   "source": [
    "===============================================\n",
    "### üéØ PURPOSE:\n",
    "This code demonstrates how to **create and use a Prompt Template** \n",
    "with Amazon Bedrock's Converse API.\n",
    "\n",
    "### üîë KEY CONCEPT:\n",
    "Templates are like reusable functions - you define them once with \n",
    "placeholders (like {{event_request}}), then \"call\" them multiple \n",
    "times by passing different values through `promptVariables`.\n",
    "\n",
    "### üìã WHAT THIS CODE DOES:\n",
    "1. Creates a prompt template for screening venue booking requests\n",
    "2. The template has a placeholder: {{event_request}}\n",
    "3. Calls the template by passing actual booking text via promptVariables\n",
    "4. Model processes the template and returns YES/NO answers\n",
    "\n",
    "### üîÑ WORKFLOW:\n",
    "Step 1: bedrock-agent.create_prompt()  ‚Üí Creates template with {{variable}}\n",
    "Step 2: bedrock.converse()             ‚Üí Calls template with actual values\n",
    "        - modelId = prompt_arn         ‚Üí Points to the template\n",
    "        - promptVariables = {...}      ‚Üí Injects values into {{variable}}\n",
    "\n",
    "### üí° ANALOGY:\n",
    "Think of it like a form letter:\n",
    "- Template: \"Dear {{name}}, welcome to {{company}}...\"  (Step 1)\n",
    "- Calling:  name=\"Alice\", company=\"AWS\"                 (Step 2)\n",
    "- Result:   \"Dear Alice, welcome to AWS...\"\n",
    "\n",
    "===============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4193ea9a-fcf1-4f4e-8b6a-67840d7b688d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created!\n",
      "arn:aws:bedrock:us-east-1:976193224059:prompt/U9MWH81BUW\n",
      "\n",
      "============================================================\n",
      "MODEL RESPONSE:\n",
      "============================================================\n",
      "A) YES - The request mentions fewer than 150 guests (90 guests).\n",
      "B) YES - The requested time is from 6pm to 10:30pm, which falls within 10am to 11pm (considering the wrap-up time).\n",
      "C) YES - The event avoids restricted activities like live animals or open flames (it will just have a DJ and dancing).\n",
      "D) YES - There is an indication that alcohol service would be handled by a licensed vendor (cousin's catering company with a liquor license).\n",
      "E) YES - There is enough information provided to follow up with the requester (date, number of guests, time frame, type of event, and details on alcohol service).\n",
      "\n",
      "Based on the information provided, the answer is YES for all criteria.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "\n",
    "# ==============================\n",
    "# Initialize AWS Clients\n",
    "# ==============================\n",
    "# bedrock_agent: For creating/managing templates\n",
    "# bedrock: For running the model (inference)\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\", region_name=\"us-east-1\")\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# Model to use\n",
    "MODEL_ID_NOVA = \"us.amazon.nova-micro-v1:0\"\n",
    "\n",
    "# ==============================\n",
    "# STEP 1: Create the Prompt Template\n",
    "# ==============================\n",
    "# This is like defining a function with parameters\n",
    "# Template has: {{event_request}} as a placeholder variable\n",
    "\n",
    "try:\n",
    "    response = bedrock_agent.create_prompt(\n",
    "        name=\"Event-Booking-Screener-1\",\n",
    "        description=\"Checks if a booking request meets venue requirements\",\n",
    "        variants=[\n",
    "            {\n",
    "                \"name\": \"Variant1\",\n",
    "                \"modelId\": MODEL_ID_NOVA,\n",
    "                \"templateType\": \"CHAT\",\n",
    "                \"inferenceConfiguration\": {\n",
    "                    \"text\": {\n",
    "                        \"temperature\": 0.4  # Low = more consistent answers\n",
    "                    }\n",
    "                },\n",
    "                \"templateConfiguration\": {\n",
    "                    \"chat\": {\n",
    "                        # System prompt: Tells model its role and instructions\n",
    "                        \"system\": [\n",
    "                            {\n",
    "                                \"text\": \"\"\"You are helping a venue manager quickly screen booking requests.\n",
    "                                Read the full message and answer YES or NO to the following.\n",
    "                                Be strict, and if you don't have enough info, just say NO.\n",
    "                                \n",
    "                                    A) Does the request mention fewer than 150 guests?\n",
    "                                    B) Is the requested time within 10am to 11pm?\n",
    "                                    C) Does the event avoid restricted activities like live animals or open flames?\n",
    "                                    D) Is there any indication that ALCOHOL service would be handled by a licensed vendor?\n",
    "                                    E) Is there enough information provided to follow up with the requester?\"\"\"\n",
    "                            }\n",
    "                        ],\n",
    "                        # User message with PLACEHOLDER variable\n",
    "                        \"messages\": [\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": [\n",
    "                                    {\n",
    "                                        # {{event_request}} will be replaced when we call the template\n",
    "                                        \"text\": \"Booking request: {{event_request}}\"\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        ],\n",
    "                        # Declare what variables this template expects\n",
    "                        \"inputVariables\": [\n",
    "                            {\"name\": \"event_request\"}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"Created!\")\n",
    "    prompt_arn = response.get(\"arn\")\n",
    "    \n",
    "except bedrock_agent.exceptions.ConflictException as e:\n",
    "    # If template already exists, just reuse it\n",
    "    print(\"Already exists!\")\n",
    "    response = bedrock_agent.list_prompts()\n",
    "    prompt = next((prompt for prompt in response['promptSummaries'] \n",
    "                   if prompt['name'] == \"Event-Booking-Screener-1\"), None)\n",
    "    prompt_arn = prompt['arn']\n",
    "\n",
    "# Print the template's unique identifier\n",
    "print(prompt_arn)\n",
    "\n",
    "# ==============================\n",
    "# STEP 2: CALL THE TEMPLATE (The Magic Part!)\n",
    "# ==============================\n",
    "# This is like calling a function: my_function(parameter=\"value\")\n",
    "# \n",
    "# HOW IT WORKS:\n",
    "# - modelId=prompt_arn          ‚Üí Instead of model ID, use TEMPLATE ARN\n",
    "# - promptVariables={...}       ‚Üí This is where we \"pass values\" to {{event_request}}\n",
    "#\n",
    "# Bedrock will:\n",
    "# 1. Load the template\n",
    "# 2. Replace {{event_request}} with the actual text below\n",
    "# 3. Send the complete prompt to the model\n",
    "# 4. Return the model's response\n",
    "\n",
    "response = bedrock.converse(\n",
    "    modelId=prompt_arn,  # ‚Üê CRITICAL: Use template ARN, not model ID!\n",
    "    promptVariables={     # ‚Üê This \"calls\" the template with actual values\n",
    "        \"event_request\": {\n",
    "            \"text\": \"\"\"Hi, I'm looking to book your event space for a birthday party. We're expecting around 90 guests,\n",
    "                and planning to start around 6pm and wrap up by 10:30pm. It will just have\n",
    "                a DJ and dancing. We'd like to serve wine and beer ‚Äî my cousin has a catering company with a liquor\n",
    "                license who would handle that. Let me know if this date is available and what next steps would be. Thanks!\n",
    "                \"\"\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# STEP 3: Display the Response\n",
    "# ==============================\n",
    "# Model will answer YES or NO to each screening question\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "print(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3fbf849-c6c4-43a4-91c8-1de3332a7486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING WITH ANOTHER BOOKING REQUEST:\n",
      "============================================================\n",
      "NO\n",
      "\n",
      "A) Does the request mention fewer than 150 guests?\n",
      "NO, the request mentions 200 people.\n",
      "\n",
      "B) Is the requested time within 10am to 11pm?\n",
      "NO, the requested time is 2am, which is outside the acceptable time frame.\n",
      "\n",
      "C) Does the event avoid restricted activities like live animals or open flames?\n",
      "NO, the request mentions bringing pet tigers, which is a restricted activity.\n",
      "\n",
      "D) Is there any indication that ALCOHOL service would be handled by a licensed vendor?\n",
      "NO, the request mentions BYOB (Bring Your Own Bottled), but it does not specify that alcohol will be handled by a licensed vendor.\n",
      "\n",
      "E) Is there enough information provided to follow up with the requester?\n",
      "NO, the request lacks sufficient details and includes several major restrictions violations.\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# üß™ BONUS: How to Reuse Template with Different Requests\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING WITH ANOTHER BOOKING REQUEST:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Same template, different values - this is the power of templates!\n",
    "response2 = bedrock.converse(\n",
    "    modelId=prompt_arn,  # Reusing same template\n",
    "    promptVariables={\n",
    "        \"event_request\": {\n",
    "            \"text\": \"\"\"Need to book space for 200 people at 2am. \n",
    "We'll bring our pet tigers for entertainment. BYOB situation.\"\"\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response2['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0becc3-9b96-4241-9f3a-fed4bc588893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
